from keras.layers import Conv3D, Activation, Input, Add, SpatialDropout3D, UpSampling3D
from keras import Model
from keras.regularizers import l2
import tensorflow_addons as tfa
import tensorflow as tf
import yaml


class CustomModel(tf.keras.Model):

    # create custom model for accumulated gradients
    def __init__(self, n_gradients, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.n_gradients = tf.constant(n_gradients, dtype=tf.int32)
        self.n_acum_step = tf.Variable(0, dtype=tf.int32, trainable=False)
        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), trainable=False) for v in
                                      self.trainable_variables]

    def train_step(self, data):
        self.n_acum_step.assign_add(1)
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

        # Calculate batch gradients
        gradients = tape.gradient(loss, self.trainable_variables)
        # Accumulate batch gradients
        for i in range(len(self.gradient_accumulation)):
            self.gradient_accumulation[i].assign_add(gradients[i])

        # If n_acum_step reach the n_gradients then we apply accumulated gradients to update the variables otherwise do nothing
        tf.cond(tf.equal(self.n_acum_step, self.n_gradients), self.apply_accu_gradients, lambda: None)

        # update metrics
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

    def apply_accu_gradients(self):
        # apply accumulated gradients
        self.optimizer.apply_gradients(zip(self.gradient_accumulation, self.trainable_variables))

        # reset
        self.n_acum_step.assign(0)
        for i in range(len(self.gradient_accumulation)):
            self.gradient_accumulation[i].assign(tf.zeros_like(self.trainable_variables[i], dtype=tf.float32))


def green_block(inp, filters, data_format='channels_first', name=None):
    """
    green_block(inp, filters, name=None)
    ------------------------------------
    Implementation of the special residual block used in the paper. The block
    consists of two (GroupNorm --> ReLu --> 3x3x3 non-strided Convolution)
    units, with a residual connection from the input `inp` to the output. Used
    internally in the model. Can be used independently as well.

    Parameters
    ----------
    `inp`: An keras.layers.layer instance, required
        The keras layer just preceding the green block.
    `filters`: integer, required
        No. of filters to use in the 3D convolutional block. The output
        layer of this green block will have this many no. of channels.
    `data_format`: string, optional
        The format of the input data. Must be either 'chanels_first' or
        'channels_last'. Defaults to `channels_first`, as used in the paper.
    `name`: string, optional
        The name to be given to this green block. Defaults to None, in which
        case, keras uses generated names for the involved layers. If a string
        is provided, the names of individual layers are generated by attaching
        a relevant prefix from [GroupNorm_, Res_, Conv3D_, Relu_, ], followed
        by _1 or _2.

    Returns
    -------
    `out`: A keras.layers.Layer instance
        The output of the green block. Has no. of channels equal to `filters`.
        The size of the rest of the dimensions remains same as in `inp`.
    """

    # axis=1 for channels_first data format
    # No. of groups = 8, as given in the paper
    x = tfa.layers.GroupNormalization(
        groups=8,
        axis=1 if data_format == 'channels_first' else 0,
        name=f'GroupNorm_1_{name}' if name else None)(inp)
    x = Activation('relu', name=f'Relu_1_{name}' if name else None)(x)
    x = Conv3D(
        filters=filters,
        kernel_size=(3, 3, 3),
        strides=1,
        padding='same',
        data_format=data_format,
        # kernel_initializer='he_normal',
        # kernel_regularizer=l2(1e-5),
        name=f'Conv3D_1_{name}' if name else None)(x)
    x = tfa.layers.GroupNormalization(
        groups=8,
        axis=1 if data_format == 'channels_first' else 0,
        name=f'GroupNorm_2_{name}' if name else None)(x)
    x = Activation('relu', name=f'Relu_2_{name}' if name else None)(x)
    x = Conv3D(
        filters=filters,
        kernel_size=(3, 3, 3),
        strides=1,
        padding='same',
        data_format=data_format,
        # kernel_initializer='he_normal',
        # kernel_regularizer=l2(1e-5),
        name=f'Conv3D_2_{name}' if name else None)(x)

    out = Add(name=f'Out_{name}' if name else None)([x, inp])  # and write inp here(instead of inp_res)
    return out


def build_model(input_shape=(4, 128, 128, 128), output_channels=3, gradient_accumulation='True',
                n_gradients=8, dice_e=1e-8):
    """
    Parameters
    ----------
    `input_shape`: A 4-tuple, optional.
        Shape of the input image. Must be a 4D image of shape (c, H, W, D),
        where, each of H, W and D are divisible by 2^4, and c is divisible by 4.
        Defaults to the crop size used in the paper, i.e., (4, 160, 192, 128).
    `output_channels`: An integer, optional.
        The no. of channels in the output. Defaults to 3 (BraTS 2018 format).
    `gradient_accumulation`: boolean, enables gradient accumulation.
    `n_gradients`: int, number of gradients in accumulated gradients.
    `dice_e`: Float, optional
        A small epsilon term to add in the denominator of dice loss to avoid dividing by
        zero and possible gradient explosion. This argument will be passed to loss_gt function.
    Returns
    -------
    `model`: A keras.models.Model instance
    """
    c, H, W, D = input_shape
    assert len(input_shape) == 4, "Input shape must be a 4-tuple"
    assert (c % 4) == 0, "The no. of channels must be divisible by 4"
    assert (H % 16) == 0 and (W % 16) == 0 and (D % 16) == 0, \
        "All the input dimensions must be divisible by 16"

    # -------------------------------------------------------------------------
    # Encoder
    # -------------------------------------------------------------------------

    # Input Layer
    inp = Input(input_shape)

    # The Initial Block
    x = Conv3D(
        filters=32,
        kernel_size=(3, 3, 3),
        strides=1,
        padding='same',
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Input_x1')(inp)

    # Dropout (0.2)
    x = SpatialDropout3D(0.2, data_format='channels_first')(x)

    # Green Block x1 (output filters = 32)
    x1 = green_block(x, 32, name='x1')

    x = Conv3D(
        filters=64,  # 64 based on 2019 winner --> 32 in code of 2018
        kernel_size=(3, 3, 3),
        strides=2,
        padding='same',
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Enc_DownSample_32')(x1)

    # Green Block x2 (output filters = 64)
    x = green_block(x, 64, name='Enc_64_1')
    x2 = green_block(x, 64, name='x2')

    x = Conv3D(
        filters=128,  # 128 based on 2019 winner --> 64 in code of 2018
        kernel_size=(3, 3, 3),
        strides=2,
        padding='same',
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Enc_DownSample_64')(x2)

    # Green Blocks x2 (output filters = 128)
    x = green_block(x, 128, name='Enc_128_1')
    x3 = green_block(x, 128, name='x3')

    x = Conv3D(
        filters=256,  # 256 based on 2019 winner --> 128 in code of 2018
        kernel_size=(3, 3, 3),
        strides=2,
        padding='same',
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Enc_DownSample_128')(x3)

    # Green Blocks x4 (output filters = 256)
    x = green_block(x, 256, name='Enc_256_1')
    x = green_block(x, 256, name='Enc_256_2')
    x = green_block(x, 256, name='Enc_256_3')
    x4 = green_block(x, 256, name='x4')

    # -------------------------------------------------------------------------
    # Decoder
    # -------------------------------------------------------------------------

    # Green Block x1 (output filters=128)
    x = Conv3D(
        filters=128,
        kernel_size=(1, 1, 1),
        strides=1,
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Dec_ReduceDepth_128')(x4)
    x = UpSampling3D(
        size=2,
        data_format='channels_first',
        name='Dec_UpSample_128')(x)
    x = Add(name='Input_Dec_128')([x, x3])

    x = green_block(x, 128, name='Dec_128')

    # Green Block x1 (output filters=64)
    x = Conv3D(
        filters=64,
        kernel_size=(1, 1, 1),
        strides=1,
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Dec_ReduceDepth_64')(x)
    x = UpSampling3D(
        size=2,
        data_format='channels_first',
        name='Dec_UpSample_64')(x)
    x = Add(name='Input_Dec_64')([x, x2])

    x = green_block(x, 64, name='Dec_64')

    # Green Block x1 (output filters=32)
    x = Conv3D(
        filters=32,
        kernel_size=(1, 1, 1),
        strides=1,
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        name='Dec_ReduceDepth_32')(x)
    x = UpSampling3D(
        size=2,
        data_format='channels_first',
        name='Dec_UpSample_32')(x)
    x = Add(name='Input_Dec_32')([x, x1])

    x = green_block(x, 32, name='Dec_32')

    # Output Block
    output = Conv3D(
        filters=3,
        kernel_size=(1, 1, 1),
        strides=1,
        data_format='channels_first',
        # kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-5),
        activation='sigmoid',
        name='Dec_Output')(x)

    # Build model
    if gradient_accumulation:
        model = CustomModel(n_gradients=n_gradients, inputs=inp, outputs=output)
    else:
        model = Model(inp, output)
    return model


if __name__ == '__main__':
    path = open('../config.yaml', 'r')
    config = yaml.safe_load(path)
    input_size = config['preprocessing_seg']['optimal_roi']
    input_size = (4, input_size[0], input_size[1], input_size[2])
    model_param = config['model']
    model = build_model(input_shape=input_size,
                        gradient_accumulation=model_param['accumulated_grad']['enable'],
                        n_gradients=model_param['accumulated_grad']['num_batch'])
    model.summary()
